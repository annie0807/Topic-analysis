{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 說明 \n",
    "\n",
    "此為TAVI領域之資料\n",
    "\n",
    "因為SQL語法過長，導致jupyter notebook開啟不順，會當機，故將語法大量刪除，有將跑好的資料存起來，從讀檔開始跑即可\n",
    "\n",
    "此處有考慮時間段、醫療辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#要存model的話 save_model = 1 ; 不存的話 save_model = 0\n",
    "save_model = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將SQL資料庫用python查詢\n",
    "\n",
    "只需將sql語法輸入query = \"\"\" 語法 \"\"\"\n",
    "\n",
    "因為要跑很久，已經有儲存資料，從下方開始跑!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconn = pymssql.connect(server=\"140.112.180.100\", user=\"sa\", password=\"Pris9630\", database=\\'Patent\\')  \\ncursor = conn.cursor()\\n#共2362筆專利\\nquery = \"\"\"SELECT a.PN\\n      ,b.[PUBD]\\n      ,[HEADING]\\n      ,[CONTENT]\\n  FROM [Patent].[dbo].[US_GRANT_FT_DESCRIPTION] a\\n  left join [Patent].[dbo].[US_GRANT_PATENT_INFO] b\\n  on a.PN =b.PN\\nwhere a.pn in(\\'D0841813\\',\\n\\'D0841812\\',\\n\\'D0812226\\',\\n\\'D0732666\\',\\n) and HEADING LIKE \\'%summary%\\' \"\"\"\\ncursor.execute(query)\\nrow = cursor.fetchone()  \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymssql \n",
    "import pickle\n",
    "'''\n",
    "conn = pymssql.connect(server=\"140.112.****.****\", user=\"*****\", password=\"**********\", database='***')  \n",
    "cursor = conn.cursor()\n",
    "#共2362筆專利\n",
    "query = \"\"\"SELECT a.PN\n",
    "      ,b.[PUBD]\n",
    "      ,[HEADING]\n",
    "      ,[CONTENT]\n",
    "  FROM [Patent].[dbo].[US_GRANT_FT_DESCRIPTION] a\n",
    "  left join [Patent].[dbo].[US_GRANT_PATENT_INFO] b\n",
    "  on a.PN =b.PN\n",
    "where a.pn in('D0841813',\n",
    "'D0841812',\n",
    "'D0812226',\n",
    "'D0732666',\n",
    ") and HEADING LIKE '%summary%' \"\"\"\n",
    "cursor.execute(query)\n",
    "row = cursor.fetchone()  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_original = pd.DataFrame(columns = ['PN', 'CONTENT','PUBD'])\n",
    "while row:  \n",
    "    df_original = df_original.append({'PN':str(row[0]), 'CONTENT':str(row[3]), 'PUBD':row[1]}, ignore_index = True)\n",
    "    #print(str(row[0]) + str(row[1]) + str(row[2]))     \n",
    "    print(df_original)\n",
    "    row = cursor.fetchone() \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('C:/Users/Annie/Desktop/Topic_analysis/patent/50_patents/var_save/df_original_TAVI.pickle', 'wb') as handle:\n",
    "#    pickle.dump(df_original, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----從這開始跑-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('var_save/df_original_TAVI.pickle', 'rb') as handle:\n",
    "    a = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_original.PN.unique #2362 -> 2264\n",
    "df_original.PN.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---這段也可以不用跑，有存df_summary---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_original['PUBD'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_summary = pd.DataFrame(columns = ['PN', 'CONTENT', 'YEAR'])\n",
    "for pn in df_original['PN'].unique():\n",
    "    content = ''\n",
    "    for i in range(len(df_original)):\n",
    "        if df_original['PN'][i] == pn:\n",
    "            year = df_original['PUBD'][i]\n",
    "            content += df_original['CONTENT'][i]\n",
    "    #print(pn,year, '\\n' ,content)\n",
    "    df_summary = df_summary.append({'PN':pn, 'CONTENT':content,'YEAR':year}, ignore_index = True)\n",
    "    # df_original = df_original.append({'PN':str(row[1]), 'CONTENT':str(row[4]), 'DATE':row[7]}, ignore_index = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "with open('C:/Users/Annie/Desktop/Topic_analysis/patent/50_patents/var_save/df_summary_TAVI.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_summary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------從這開始繼續!---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('var_save/df_summary_TAVI.pickle', 'rb') as handle:\n",
    "    df_summary = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PN</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07166570</td>\n",
       "      <td>Briefly stated, the present invention provides...</td>\n",
       "      <td>2007-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07175656</td>\n",
       "      <td>The present invention provides improved device...</td>\n",
       "      <td>2007-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07195640</td>\n",
       "      <td>The medical devices in combination with therap...</td>\n",
       "      <td>2007-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07241295</td>\n",
       "      <td>It is an object of the invention to provide a ...</td>\n",
       "      <td>2007-07-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07261732</td>\n",
       "      <td>Accordingly, an object of the present inventio...</td>\n",
       "      <td>2007-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>10849745</td>\n",
       "      <td>Embodiments hereof relate to a balloon cathete...</td>\n",
       "      <td>2020-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>08038710</td>\n",
       "      <td>The present invention provides artificial valv...</td>\n",
       "      <td>2011-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>10729542</td>\n",
       "      <td>As discussed above, stented prosthetic heart v...</td>\n",
       "      <td>2020-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>09801721</td>\n",
       "      <td>In some embodiments, a sizing device for use i...</td>\n",
       "      <td>2017-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>10959848</td>\n",
       "      <td>An exemplary implantable prosthetic device can...</td>\n",
       "      <td>2021-03-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PN                                            CONTENT        YEAR\n",
       "0     07166570  Briefly stated, the present invention provides...  2007-01-23\n",
       "1     07175656  The present invention provides improved device...  2007-02-13\n",
       "2     07195640  The medical devices in combination with therap...  2007-03-27\n",
       "3     07241295  It is an object of the invention to provide a ...  2007-07-10\n",
       "4     07261732  Accordingly, an object of the present inventio...  2007-08-28\n",
       "...        ...                                                ...         ...\n",
       "2259  10849745  Embodiments hereof relate to a balloon cathete...  2020-12-01\n",
       "2260  08038710  The present invention provides artificial valv...  2011-10-18\n",
       "2261  10729542  As discussed above, stented prosthetic heart v...  2020-08-04\n",
       "2262  09801721  In some embodiments, a sizing device for use i...  2017-10-31\n",
       "2263  10959848  An exemplary implantable prosthetic device can...  2021-03-30\n",
       "\n",
       "[2264 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新增手動查詢的summary\n",
    "\n",
    "如果沒有人工查詢其他summary資料，可以不用執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#有手動新增資料 == 1 ;沒有 == 0\n",
    "add_summary = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_summary == 1:\n",
    "    df_loss = pd.read_excel('DESCRIPTION_SUMMARY_0521.xlsx','工作表1')  #增加手動新增的summary，注意格式要PN / summary / PUBD\n",
    "    documents_merge = df_summary.append(df_loss,ignore_index = True)\n",
    "else:\n",
    "    documents_merge = df_summary\n",
    "\n",
    "documents_merge['YEAR'] = pd.to_datetime(documents_merge['YEAR']).dt.date #不顯示時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year06_11 = []\n",
    "year11_16 = []\n",
    "year16_21 = []\n",
    "for i in range(len(documents_merge.YEAR)):\n",
    "    if documents_merge.iloc[i][2] <= date(2010, 12, 31):\n",
    "        year06_11.append(str(documents_merge.iloc[i][1]))\n",
    "    elif documents_merge.iloc[i][2] <= date(2015, 12, 31):\n",
    "        year11_16.append(str(documents_merge.iloc[i][1]))\n",
    "    elif documents_merge.iloc[i][2] <= date(2021, 12, 31):\n",
    "        year16_21.append(str(documents_merge.iloc[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(year06_11), len(year11_16), len(year16_21))\n",
    "#13 -> 12\n",
    "#18 -> 13 -> 15\n",
    "#19 -> 14 -> 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(year06_11, columns = ['summary'])\n",
    "df_2 = pd.DataFrame(year11_16, columns = ['summary'])\n",
    "df_3 = pd.DataFrame(year16_21, columns = ['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedTerm\n",
    "\n",
    "是否要用UMLS!????????????????→ 先不用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('MedTerm.txt')\n",
    "text = []\n",
    "MedTerm = []\n",
    "for line in f:\n",
    "    text.append(line)\n",
    "for i in text:\n",
    "    MedTerm.append(i.split('\\n', 1)[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize_stemming('amini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    # 把字詞幹化\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess_term(text):\n",
    "    term_lemma = []\n",
    "    term_tmp = []\n",
    "    result = [] \n",
    "    #print(text)\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text, min_len=min_len, max_len=35):  #tokenize\n",
    "        #print(token)\n",
    "        term_tmp.append((lemmatize_stemming(token)))\n",
    "    term_lemma.append(' '.join(term_tmp))\n",
    "    return term_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MedTerm_lemma = []  #儲存詞幹化之後的MedTerms\n",
    "for w in MedTerm:\n",
    "    #print(preprocess(w))\n",
    "    MedTerm_lemma.append(preprocess_term(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MedTerm_lemma_fin = []   #將MedTerms弄成跟原本的MedTerms一樣格式\n",
    "for i in range(len(MedTerm_lemma)):\n",
    "    for j in range(len(MedTerm_lemma[i])):\n",
    "        MedTerm_lemma_fin.append(MedTerm_lemma[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # import MWETokenizer() method from nltk \n",
    "from nltk.tokenize import MWETokenizer \n",
    "   \n",
    "# Create a reference variable for Class MWETokenizer  \n",
    "\n",
    "MedTerm_mwe = []  #將兩個以上單字組成的字詞用tuple刮在一起\n",
    "#tmp = ()\n",
    "for i in range(len(MedTerm_lemma_fin)):\n",
    "    if len(MedTerm_lemma_fin[i].split(' '))> 1:\n",
    "        tmp = ()\n",
    "        tmp = tmp + tuple(MedTerm_lemma_fin[i].split(' '))\n",
    "        MedTerm_mwe.append(tmp)\n",
    "    else:\n",
    "        MedTerm_mwe.append(MedTerm_lemma_fin[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#兩個以上的字用_連接\n",
    "tk = MWETokenizer(MedTerm_mwe) \n",
    "MedTerm_token = []\n",
    "MedTerm_token_show = []  #將長度=0的也留著(為了方便表格觀看)\n",
    "for i in range(len(MedTerm_lemma_fin)):\n",
    "    if len(MedTerm_lemma_fin[i]) != 0:\n",
    "        MedTerm_token.append(tk.tokenize(MedTerm_lemma_fin[i].split())[0])\n",
    "        MedTerm_token_show.append(tk.tokenize(MedTerm_lemma_fin[i].split())[0])       \n",
    "    else:\n",
    "        MedTerm_token_show.append('')\n",
    "    #print(token_medterm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in MedTerm_token:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    # 把字詞幹化\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    docu_tmp = []\n",
    "    documents_lemma = []\n",
    "    result = [] \n",
    "    stop_word = ['ul', 'id', 'sub']\n",
    "    for token in gensim.utils.simple_preprocess(text, min_len=min_len, max_len=35):  #tokenize\n",
    "        docu_tmp.append(lemmatize_stemming(token))\n",
    "    documents_lemma.append(' '.join(docu_tmp))\n",
    "\n",
    "    text_token = tk.tokenize(documents_lemma[0].split())\n",
    "    for token in text_token:\n",
    "        if token in MedTerm_token and token not in gensim.parsing.preprocessing.STOPWORDS and token not in stop_word:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分三個年份\n",
    "processed_docs_1 = df_1['summary'].map(preprocess)\n",
    "processed_docs_2 = df_2['summary'].map(preprocess)\n",
    "processed_docs_3 = df_3['summary'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不分年分\n",
    "processed_docs = documents_merge['CONTENT'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words on the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dictionary.iteritems():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_1 = gensim.corpora.Dictionary(processed_docs_1)\n",
    "dictionary_2 = gensim.corpora.Dictionary(processed_docs_2)\n",
    "dictionary_3 = gensim.corpora.Dictionary(processed_docs_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary_1)\n",
    "print(dictionary_2)\n",
    "print(dictionary_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dictionary_1.iteritems():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dictionary_2.iteritems():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dictionary_3.iteritems():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_1.filter_extremes(no_below=0, no_above=0.5, keep_n=100000)\n",
    "dictionary_2.filter_extremes(no_below=0, no_above=0.5, keep_n=100000)\n",
    "dictionary_3.filter_extremes(no_below=0, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary_1)\n",
    "print(dictionary_2)\n",
    "print(dictionary_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim doc2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus_1 = [dictionary_1.doc2bow(doc) for doc in processed_docs_1]\n",
    "bow_corpus_2 = [dictionary_2.doc2bow(doc) for doc in processed_docs_2]\n",
    "bow_corpus_3 = [dictionary_3.doc2bow(doc) for doc in processed_docs_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_docs = documents_merge['TACD'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words on the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.filter_extremes(no_below=3, no_above=0.5, keep_n=100000)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim doc2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimal number of topics for LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=50, workers=8)\n",
    "        #model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list_1, coherence_values_1 = compute_coherence_values(dictionary=dictionary_1, corpus=bow_corpus_1, texts=processed_docs_1, start=2, limit=9, step=1)\n",
    "model_list_2, coherence_values_2 = compute_coherence_values(dictionary=dictionary_2, corpus=bow_corpus_2, texts=processed_docs_2, start=2, limit=9, step=1)\n",
    "model_list_3, coherence_values_3 = compute_coherence_values(dictionary=dictionary_3, corpus=bow_corpus_3, texts=processed_docs_3, start=2, limit=9, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "limit=9; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values_1)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values_2)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values_3)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values_1):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4)) #2\n",
    "    \n",
    "for m, cv in zip(x, coherence_values_2):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4)) #3\n",
    "\n",
    "for m, cv in zip(x, coherence_values_3):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4)) #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "from pprint import pprint\n",
    "optimal_model_1 = model_list_1[3]\n",
    "model_topics = optimal_model_1.show_topics(formatted=False)\n",
    "#pprint(optimal_model.print_topics(num_words=10))\n",
    "\n",
    "optimal_model_2 = model_list_2[3]\n",
    "model_topics = optimal_model_2.show_topics(formatted=False)\n",
    "#pprint(optimal_model.print_topics(num_words=10))\n",
    "\n",
    "optimal_model_3 = model_list_3[-3]\n",
    "model_topics = optimal_model_3.show_topics(formatted=False)\n",
    "#pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import MmCorpus\n",
    "if save_model == 1:\n",
    "    MmCorpus.serialize('patent_1.mm', bow_corpus_1)\n",
    "    dictionary_1.save('patent_1.dict')\n",
    "    MmCorpus.serialize('patent_2.mm', bow_corpus_2)\n",
    "    dictionary_2.save('patent_2.dict')\n",
    "    MmCorpus.serialize('patent_3.mm', bow_corpus_3)\n",
    "    dictionary_3.save('patent_3.dict')\n",
    "    optimal_model_1.save('patent_1.model')\n",
    "    optimal_model_2.save('patent_2.model')\n",
    "    optimal_model_3.save('patent_3.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=100, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherencemodel = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "#coherencemodel.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "vis_data_1 = gensimvis.prepare(topic_model=optimal_model_1, corpus=bow_corpus_1, dictionary=optimal_model_1.id2word)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_data_2 = gensimvis.prepare(topic_model=optimal_model_2, corpus=bow_corpus_2, dictionary=optimal_model_2.id2word)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data_3 = gensimvis.prepare(topic_model=optimal_model_3, corpus=bow_corpus_3, dictionary=optimal_model_3.id2word)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis_data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
